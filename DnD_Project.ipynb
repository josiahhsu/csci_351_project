{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model imports\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing our processed data, dropping columns unhelpful for classification, and separating the features from the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnd = pd.read_csv(\"dnd_monsters_processed.csv\").drop(columns=[\"Index\", \"name\"])\n",
    "features = dnd.drop(columns=[\"cr\"])\n",
    "target = dnd.cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the complete dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a visual for how many monsters are in each challenge rating class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = dnd.cr.value_counts().sort_index().plot(kind='bar', figsize=(14,8),title=\"Count of Challenge Ratings\")\n",
    "ax.set_xlabel(\"Challenge Ratings\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our machine learning models we'll need to convert the features and targets to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.to_numpy()\n",
    "y = target.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not all models can take real-numbered targets, we also create a shifted target vector that converts the fractional challenge ratings into the lowest-valued integers and shift the rest of the challenge ratings to accommodate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shift = []\n",
    "for i in y:\n",
    "    if i == .125:\n",
    "        y_shift.append(1)\n",
    "    elif i == .25:\n",
    "        y_shift.append(2)\n",
    "    elif i == .5:\n",
    "        y_shift.append(3)\n",
    "    else:\n",
    "        y_shift.append(i+3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using the whole dataset, we thought that it would be interesting to partition the dataset into different categories and test how much influence each category had. When examining the data, three main partitions stood out: general traits, mobility capabilities, and ability scores. To examine their individual impacts we created an array of modified observations with different combinations of these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traits = [\"size\", \"type\", \"alignment\", \"legendary\"]\n",
    "mobility = [\"speed\", \"swim\", \"climb\", \"fly\", \"burrow\"]\n",
    "abilityscore = [\"ac\", \"hp\",\"strength\", \"dex\", \"con\", \"intel\", \"wis\", \"cha\"]\n",
    "\n",
    "names = [\"No traits\", \"No mobility\", \"No ability score\", \"Only traits\", \"Only mobility\", \"Only ability score\"]\n",
    "X_mod = [features.drop(columns=traits)]\n",
    "X_mod.append(features.drop(columns=mobility))\n",
    "X_mod.append(features.drop(columns=abilityscore))\n",
    "X_mod.append(features.drop(columns=mobility+abilityscore))\n",
    "X_mod.append(features.drop(columns=abilityscore+traits))\n",
    "X_mod.append(features.drop(columns=traits+mobility))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run our experiments, we created a helper function that takes a model, the observations, target values, and a number of trials to run. This allowed us to easily run experiments per model and find an average score for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X, y, trials):\n",
    "    test = []\n",
    "    train = []\n",
    "    for i in range(trials):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)    \n",
    "        model.fit(X_train, y_train)\n",
    "        train.append(model.score(X_train, y_train))\n",
    "        test.append(model.score(X_test, y_test))\n",
    "    return (np.mean(train), np.mean(test))\n",
    "\n",
    "# wrapper for nice printing\n",
    "def print_test_model(model, X, y, trials):\n",
    "    train_mean, test_mean = test_model(model, X, y, trials)\n",
    "    print(\"Accuracy on training set: {:.5f}\".format(train_mean))\n",
    "    print(\"Accuracy on test set: {:.5f}\\n\".format(test_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also created a helper function to test how the model performs when using the various feature partitions. Since the partitions were based on human judgment we kept this one simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_mod(model, names, X, y, trials):\n",
    "    for i in range(len(names)):\n",
    "        print(names[i])\n",
    "        print_test_model(model, X[i], y, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we created a function to determine which targets the model missed. Instead of running multiple trials, these functions run a model just once. The most notable feature here is the margin of error checker, which gives a nice picture of how far off the model was on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an array of (predict, true) pairs for all misclassifications\n",
    "def find_missed(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return np.array([[y_pred[i], y[i]] for i in range(len(y)) if y_pred[i] != y[i] ])\n",
    "        \n",
    "def test_missed_model(model, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)    \n",
    "    model.fit(X_train, y_train)\n",
    "    train = model.score(X_train, y_train)\n",
    "    test = model.score(X_test, y_test)\n",
    "    \n",
    "    train_missed = find_missed(model, X_train, y_train)\n",
    "    test_missed = find_missed(model, X_test, y_test)\n",
    "    \n",
    "    return (train, train_missed, test, test_missed)\n",
    "\n",
    "# Finds the average margin of error for a model\n",
    "def find_margin(missed):\n",
    "    margin = 0\n",
    "    for pred, true in missed:\n",
    "        margin += abs(pred-true)\n",
    "    return margin/len(missed)\n",
    "\n",
    "# Helper print functions #\n",
    "\n",
    "# Print first n incorrect predictions\n",
    "def print_missed(missed, n):\n",
    "    print(\"Number of incorrect predictions: {}\".format(len(missed)))\n",
    "    print(\"Average margin of error: {}\".format(find_margin(missed)))\n",
    "    for pred, true in missed[:n]:\n",
    "        print(\"Predicted {}, was {}\".format(pred, true))\n",
    "    print()\n",
    "    \n",
    "def print_test_missed_model(model, X, y, n):\n",
    "    train, train_missed, test, test_missed = test_missed_model(model, X, y)\n",
    "    print(\"Accuracy on training set: {:.5f}\".format(train))\n",
    "    print_missed(train_missed, n)\n",
    "    print(\"Accuracy on test set: {:.5f}\".format(test))\n",
    "    print_missed(test_missed, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model family, we start by running the test_model function on all the features to create a baseline for expected accuracy. Next, we do a sample run to get an idea of what the model predicts incorrectly. After that, we experiment with different model parameters as applicable. Finally, we run the model against the different feature partitions to see if those have any influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, we first started by trying to use knn classification and regression schemes. Since knn models cannot use continuous target values, we had to use the shifted targets. Let's start with trying out classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(5)\n",
    "print_test_model(knn, X, y_shift, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch. Not good. Let's take a look at some of the misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(5)\n",
    "print_test_missed_model(knn, X, y_shift, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty significant number of misclassifications and a high margin of error. Let's see how regression fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_reg = KNeighborsRegressor(5)\n",
    "print_test_model(knn_reg, X, y_shift, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already that looks much better. Let's see what the incorrect predictions are like this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_reg = KNeighborsRegressor(5)\n",
    "print_test_missed_model(knn_reg, X, y_shift, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have many more incorrect predictions, but the margin of error has decreased by half. This smaller margin is the reason why regression scores much better overall, as the incorrect regression predictions are off by much less compared to the classification predictions. This implies that classification schemes are less appropriate for our task because they force non-continuous predictions and thus will have wider margins of error.\n",
    "\n",
    "Since regression is more appropriate for determining challenge rating, we choose to focus on regression model families for the rest of our experiments. But first, let's see if we can improve the knn results by changing the number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_max = 0\n",
    "best_pos = 0\n",
    "\n",
    "for i in range(1, 20):\n",
    "    knn_reg = KNeighborsRegressor(i)\n",
    "    train_mean, test_mean = test_model(knn_reg, X, y_shift, 20)\n",
    "    if (test_mean > test_max):\n",
    "        test_max = test_mean\n",
    "        best_pos = i\n",
    "\n",
    "print(\"Best pos: {}\".format(best_pos))\n",
    "print(\"Best test mean: {}\".format(test_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating this test indicated that the best results typically occur between 12 and 18 neighbors. Sinec the accuracy was pretty consistent across tests, we'll split the difference and use 15. Now let's test this against the different partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_reg = KNeighborsRegressor(15)\n",
    "test_model_mod(knn_reg, names, X_mod, y_shift, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next set of tests uses the linear regression model. This ran much faster compared to knn regression, but had a slight hit to accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "print_test_model(lr, X, y, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the missed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "print_test_missed_model(lr, X, y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the modified features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "test_model_mod(lr, names, X_mod, y, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move on to the multilevel perception model. Before we run the tests, we're going to ignore warnings. The MLPRegressor will warn us whenever the model doesn't converge, which gets really distracting. Later we'll adjust the maximum number of iterations for the model, which can also eliminate the convergence warnings, but for presentation purposes we'll create the filter beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to actually test the MLP. This was the slowest to run of them all, so we had to reduce the number of trials to get results in a reasonable amount of time. As a consequence there was some more variance compared to when we ran trials on the other models, but MLPs still had the highest accuracy overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor()\n",
    "print_test_model(mlp, X, y, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a slight improvement over the other models. Let's see what it gets wrong..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor()\n",
    "print_test_missed_model(mlp, X, y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it's a good baseline, but let's see if we can improve it by fiddling with some parameters. Many are solver-specific, but the following are universal parameters that we can adjust. \n",
    "\n",
    "First, the number of hidden layers generally improved the training accuracy, but didn't do much for the test set. If anything, test set accuracy went down slightly, which may be an indication of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is [100]\n",
    "hidden_layer_count = [[100]*i for i in range(1, 6)]\n",
    "for s in hidden_layer_count:\n",
    "    print(\"Hidden layers: {}\".format(s))\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=s)\n",
    "    print_test_model(mlp, X, y, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of hidden units in a layer followed the same trend, but the test set accuracy didn't suffer as much. In fact, by the end it improved slightly, though it's unclear how much of that came from random variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is [100]\n",
    "hidden_layer_sizes = [[100*i] for i in range(1, 11)]\n",
    "for s in hidden_layer_sizes:\n",
    "    print(\"Hidden layer: {}\".format(s))\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=s)\n",
    "    print_test_model(mlp, X, y, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default relu activation function works the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is relu\n",
    "activation = ['relu', 'identity', 'logistic', 'tanh']\n",
    "for a in activation:\n",
    "    print(\"Activation: \" + a)\n",
    "    mlp = MLPRegressor(activation=a)\n",
    "    print_test_model(mlp, X, y, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default adam solver has the best test accuracy, but lbfgs did have better training accuracy. The sgd solver doesn't work at all though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is adam\n",
    "solvers = ['adam', 'lbfgs', 'sgd']\n",
    "for s in solvers:\n",
    "    print(\"Solver: \" + s)\n",
    "    mlp = MLPRegressor(solver=s)\n",
    "    print_test_model(mlp, X, y, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the alpha had some fluxuations but didn't result in any significant improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is 0.0001\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "for a in alphas:\n",
    "    print(\"Alpha: {}\".format(a))\n",
    "    mlp = MLPRegressor(alpha=a)\n",
    "    print_test_model(mlp, X, y, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, increasing the iteration count to allow for convergence trended towards higher overall accuracy, though this seems to be more of a question of consistency than actual improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is 200\n",
    "count = [200, 400, 600, 800, 1000]\n",
    "for c in count:\n",
    "    print(\"count: {}\".format(c))\n",
    "    mlp = MLPRegressor(max_iter=c)\n",
    "    print_test_model(mlp, X, y, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a whole, it looks like changing individual parameters can only improve accuracy by around 1% at most. Let's try combining the best-performing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=[800, 800], max_iter=1500)\n",
    "print_test_model(mlp, X, y, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe a bit better, but not by much. While we could likely fine-tune the parameters to eek out more overall accuracy, it seems unlikely that it'll improve by more than a percent or two. The biggest change appears to be the amount of time the model takes to train, which may not be worth it for such small improvements.\n",
    "\n",
    "Let's go back to the default parameters and look at the partitioned features once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor()\n",
    "test_model_mod(mlp, names, X_mod, y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the models we testd, MLPs performed the best.\n",
    "\n",
    "One possible consideration for why accuracy seems to be stuck in the 90-95% range is the distribution of challenge score ratings. As we saw with the initial graph, there's far more monsters on the lower end of the spectrum than high-ranking ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
